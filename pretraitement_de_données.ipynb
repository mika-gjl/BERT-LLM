{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2cd25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered_landmarktype.jsonl\"\n",
    "\n",
    "def fix_landmark_type(triples):\n",
    "    new_triples = []\n",
    "    for triple in triples:\n",
    "        if triple.get(\"rel\") == \"isLandmarkType\":\n",
    "            # 取地名类型前缀（如 \"Rue\"）\n",
    "            sub = triple.get(\"sub\", \"\")\n",
    "            prefix = sub.split()[0] if sub else \"\"\n",
    "            # 新 triple\n",
    "            new_triples.append({\n",
    "                \"sub\": prefix,\n",
    "                \"rel\": \"isLandmarkTypeOf\",\n",
    "                \"obj\": sub.lower()\n",
    "            })\n",
    "        else:\n",
    "            new_triples.append(triple)\n",
    "    return new_triples\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            data[\"triples\"] = fix_landmark_type(data[\"triples\"])\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b512fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "\n",
    "def extract_full_dates(text):\n",
    "    \"\"\"\n",
    "    Extraire toutes les expressions de dates complètes dans la phrase (retourne une liste de segments d'origine, format français)\n",
    "    \"\"\"\n",
    "    # Correspond à \"1er février 1877\", \"2 mars 1912\", \"01 avril 1855\", etc.\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    results = []\n",
    "    for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "        phrase = text[m.start():m.end()]\n",
    "        # On ne garde que celles qui peuvent être correctement analysées par dateparser\n",
    "        dt = dateparser.parse(phrase, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            results.append(phrase.strip())\n",
    "    return results\n",
    "\n",
    "def replace_incomplete_dates(sent, triples):\n",
    "    # Extraire toutes les dates complètes dans la phrase (ex : « 1er février 1877 »)\n",
    "    full_dates = extract_full_dates(unidecode(sent))\n",
    "    # Construire une table de correspondance { (année, mois): expression d'origine }\n",
    "    mapping = {}\n",
    "    for d in full_dates:\n",
    "        dt = dateparser.parse(d, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            key = (str(dt.year), str(dt.month).zfill(2))\n",
    "            mapping[key] = d.strip()\n",
    "\n",
    "    # Traiter chaque triple individuellement\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", obj)\n",
    "        if m:\n",
    "            year, month = m.group(1), m.group(2)\n",
    "            if (year, month) in mapping:\n",
    "                # Remplacer par l'expression temporelle réelle trouvée dans la phrase\n",
    "                triple[\"obj\"] = mapping[(year, month)]\n",
    "        # Pour traiter aussi les formats yyyy-mm-dd si besoin\n",
    "        m2 = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", obj)\n",
    "        if m2:\n",
    "            year, month, day = m2.group(1), m2.group(2), int(m2.group(3))\n",
    "            for dstr, dphrase in mapping.items():\n",
    "                if dstr == (year, month):\n",
    "                    if str(day) in dphrase or f\"{day:02d}\" in dphrase or (day == 1 and \"1er\" in dphrase):\n",
    "                        triple[\"obj\"] = dphrase\n",
    "    return triples\n",
    "\n",
    "# ====== Traitement batch du fichier ======\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered_landmarktype.jsonl\"    # Chemin du fichier d'entrée\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"  # Chemin du fichier de sortie\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = replace_incomplete_dates(d[\"sent\"], d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7eb25679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "def fr_date_to_iso(date_str):\n",
    "    # Convertit une date en français (« 1er février 1877 », etc.) au format ISO (« 1877-02-01 »)\n",
    "    dt = dateparser.parse(date_str, languages=['fr'])\n",
    "    if dt:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return date_str\n",
    "\n",
    "def update_triples_date_obj(triples):\n",
    "    # Pour chaque triple, si l'objet est une date complète en français, on la remplace par le format ISO\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        # Vérifie si l'objet est une date en français (« 1er février 1877 », « 21 mai 1956 », etc.)\n",
    "        if re.fullmatch(pattern, obj, re.IGNORECASE):\n",
    "            iso_date = fr_date_to_iso(obj)\n",
    "            triple[\"obj\"] = iso_date\n",
    "    return triples\n",
    "\n",
    "# ========== Traitement batch du fichier JSONL ==========\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = update_triples_date_obj(d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "606801eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ： C:/Users/jguo/Desktop/PURE-main/data/test_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prétraitement du jeu de donées pour supprimer des triplets raisonnés\n",
    "#\"sub\": \"Boulevard de Sébastopol\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"noTime\"\n",
    "# si le sujet et l'objet entre la relation \"hasNewName\" sont pareils, on supprime le triplet. {\"sub\": \"Pont Alexandre III\", \"rel\": \"hasNewName\", \"obj\": \"pont Alexandre III\"}\n",
    "import json\n",
    "\n",
    "# changer le chemin d'accès aux fichiers d'entrée et de sortie\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            new_triples = []\n",
    "            for triple in data[\"triples\"]:\n",
    "                rel = triple.get(\"rel\", \"\")\n",
    "                obj = triple.get(\"obj\", \"\")\n",
    "                sub = triple.get(\"sub\", \"\")\n",
    "\n",
    "                # 1：sauter obj == noTime\n",
    "                if obj == \"noTime\":\n",
    "                    continue\n",
    "                # 2：sauter hasNewName et sub.lower() == obj.lower()\n",
    "                if rel == \"hasNewName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "                if rel == \"hasOldName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "\n",
    "                # garder ce triplet\n",
    "                new_triples.append(triple)\n",
    "\n",
    "            # actualiser les triplets\n",
    "            data[\"triples\"] = new_triples\n",
    "\n",
    "        # Chaque ligne de sortie reste un dictionnaire JSON.\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\" Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ：\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f562cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 12 relations. Saved to C:/Users/jguo/Documents/BERT-LLM/dataset/pegazus-event-extraction/rel2id.json\n"
     ]
    }
   ],
   "source": [
    "### générer le fichier de rel2id.json\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "input_path = \"C:/Users/jguo/Documents/BERT-LLM/dataset/pegazus-event-extraction/new_train.jsonl\"     # 替换为你的输入 JSONL 路径\n",
    "output_path = \"C:/Users/jguo/Documents/BERT-LLM/dataset/pegazus-event-extraction/rel2id.json\"   # 输出文件\n",
    "\n",
    "rels = set()\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        for t in obj.get(\"triples\", []):\n",
    "            rel = (t.get(\"rel\") or \"\").strip()\n",
    "            if rel:\n",
    "                rels.add(rel)\n",
    "\n",
    "# 稳定且可复现：按字母序排序\n",
    "rel_list = sorted(rels)\n",
    "\n",
    "rel2id = OrderedDict((rel, idx) for idx, rel in enumerate(rel_list))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rel2id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Collected {len(rel2id)} relations. Saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
